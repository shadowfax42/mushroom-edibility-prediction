# -*- coding: utf-8 -*-
"""DMSL_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18rJEc9_53qimx55suKxlsmHn9p3rOYRP
"""

# pip install powershap

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report
import itertools
from sklearn.linear_model import Ridge, Lasso, ElasticNet, RidgeCV, LassoCV
from sklearn import metrics
import seaborn as sns
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.metrics import f1_score
from sklearn.metrics import precision_recall_fscore_support as score
from matplotlib.pyplot import figure
import random
from sklearn.linear_model import ElasticNetCV, LassoCV
from sklearn.inspection import permutation_importance
import missingno as msno
import pydot
from IPython.display import Image
# import shap
# from powershap import PowerShap

"""## Data Preparation <a class="anchor" id="dataPreparation"></a>
This is data preparation
"""

# Mount the drive
from google.colab import drive
drive.mount("/content/drive")

# data = pd.read_csv("/content/drive/MyDrive/DMSL project/MushroomDataset/secondary_data.csv", sep=";")
data = pd.read_csv("/content/drive/MyDrive/DMSL project/MushroomDataset/mushroom_with_levels.csv")
data.head()

# summary of dataframe
data.info()

"""### EDA <a class="anchor" id="eda"></a>
This is exploratory data analysis
"""

# convert stem.height and cap.diameter to mm
data['stem.height'] = data['stem.height'].apply(lambda x: float(x*10))
data['cap.diameter'] = data['cap.diameter'].apply(lambda x: float(x*10))

# check missing values
def get_missing_values(df):

  percent_missing = df.isnull().sum() * 100 / len(df)
  missing_value_df = pd.DataFrame({'Column Name': df.columns,
                                  '% Missing values': round(percent_missing, 2)}).reset_index(drop=True)

  missing_value_df.sort_values('% Missing values', ascending=False, inplace=True)
  return missing_value_df.reset_index(drop=True)


get_missing_values(data)

from pandas.plotting import table
fig, ax = plt.subplots(figsize=(12, 2)) # set size frame
ax.xaxis.set_visible(False)  # hide the x axis
ax.yaxis.set_visible(False)  # hide the y axis
ax.set_frame_on(False)  # no visible frame, uncomment if size is ok
tabla = table(ax, data.head(), loc='upper right', colWidths=[0.17]*len(data.columns))  # where df is your data frame
tabla.auto_set_font_size(False) # Activate set fontsize manually
tabla.set_fontsize(12) # if ++fontsize is necessary ++colWidths
tabla.scale(1.5, 1.5) # change size table
plt.savefig('/content/drive/MyDrive/DMSL project/MushroomDataset/data.png', transparent=True)

# Gives a bar chart of the missing values
msno.bar(data, color="limegreen")
plt.show()
plt.savefig('/content/drive/MyDrive/DMSL project/MushroomDataset/missingVals.png', transparent=True)

# Shows where the data is missing.  It also has the sparkline on the side where it shows there are minimum of 13 colummns that always have data and a maximum of 19 columns with data less often
msno.matrix(data, fontsize=12, sparkline=False, color=(0.494, 0.184, 0.556))
plt.show()
plt.savefig('/content/drive/MyDrive/DMSL project/MushroomDataset/missingValsMatrix.png', transparent=True)

# Gives a heatmap of how missing values are related
msno.heatmap(data, cmap="flare")
plt.show()

''' Shows the correlation between the missing data columns.  This uses a hierarchical clustering algorithm to bin variables against one another by their nullity correlation. 
Cluster leaves which linked together at a distance of zero fully predict one anotherâ€™s presence. https://mathdatasimplified.com/2021/10/04/missingno-dendogram-visualize-correlation-between-missing-data/
'''
msno.dendrogram(data)
plt.show()

"""### Outlier detection in continous variables"""

fig = plt.figure(figsize =(20, 10))
plt.rcParams.update({'font.size': 16})
sns.boxplot(x="variable", y="value", data=pd.melt(data[["cap.diameter", "stem.width", "stem.height"]]))
plt.title("Box plot of non-categorical variables (units in mm)", fontsize=16)
plt.show()

# IQR, above upper and lower bounds
def quartiles_iqr(df, colname):

  Q1 = np.percentile(df[colname], 25, interpolation = 'midpoint') 
  Q3 = np.percentile(df[colname], 75, interpolation = 'midpoint') 
  IQR = Q3 - Q1
  
  return Q1, Q3, IQR

def get_upper_lower_bounds(df, colname):
  Q1, Q3, IQR = quartiles_iqr(df, colname)

  # Above Upper bound
  upper = df[colname] >= (Q3 + 1.5 * IQR)

  # Below Lower bound
  lower = df[colname] <= (Q1 - 1.5 * IQR)

  return upper, lower

upper_cd, lower_cd = get_upper_lower_bounds(data, "cap.diameter")

print("Upper bound:",upper_cd)
print(np.where(upper_cd))

print("Lower bound:", lower_cd)
print(np.where(lower_cd))

upper_sw, lower_sw = get_upper_lower_bounds(data, "stem.width")

print("Upper bound:",upper_sw)
print(np.where(upper_sw))

print("Lower bound:", lower_sw)
print(np.where(lower_sw))

upper_sh, lower_sh = get_upper_lower_bounds(data, "stem.height")

print("Upper bound:",upper_sh)
print(np.where(upper_sh))

print("Lower bound:", lower_sh)
print(np.where(lower_sh))

def get_outlier_indices(df, columns=['cap.diameter', 'stem.width', 'stem.height']):
	# initialize an empty dictionary
	outlier_indices = {}
	
	for col in columns:
		# get upper and lower bounds
		upper, lower = get_upper_lower_bounds(df, col)
		
		# get indices of each
		indices = np.where(upper) + np.where(lower)
		
		# populate the dictionary
		outlier_indices[col] = indices
	
	return outlier_indices

outliers_arrays = get_outlier_indices(data)
arr1 = outliers_arrays['cap.diameter'][0]
arr2 = outliers_arrays['stem.height'][0]
arr3 = outliers_arrays['stem.width'][0]
outliers_indices = np.concatenate((arr1,arr2,arr3))
outliers = list(set(outliers_indices))
len(outliers)

# drop columns with 65% or more missing values
df = data.copy()
df_trimmed = df.dropna(thresh=df.shape[0]*0.65, how='all',axis=1)
cols = df_trimmed.columns

# plotting
color_palette_list = ['#009ACD', '#ADD8E6']
df.groupby('class').size().plot(kind='pie', y='class', autopct='%1.0f%%',colors=color_palette_list, explode=[0, 0.1])
plt.show()
plt.savefig('/content/drive/MyDrive/DMSL project/MushroomDataset/classProportion.png', transparent=True)

"""### Missing value  Imputation"""

# imputing missing values using the most frequent method

# impute missing values using KNN
def most_freq_imputation(data):
  from sklearn.impute import SimpleImputer

  imputer = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')
  return imputer.fit_transform(data)

df_imput = most_freq_imputation(df_trimmed)
df_imputted = pd.DataFrame(df_imput, columns=cols)

get_missing_values(df_imputted)

# cast numeric columns

df_imputted[['cap.diameter', 'stem.width', 'stem.height']] =  df_imputted[['cap.diameter', 'stem.width', 'stem.height']].apply(pd.to_numeric)

df_imputted.to_csv("/content/drive/MyDrive/DMSL project/MushroomDataset/output.csv")

# train test split
X = df_imputted.drop(['class'], axis=1)
y = df_imputted.iloc[:, 0]

"""### Encoding categorical data"""

# split the dataset into train and test sets
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

categorical_cols = ['cap.shape', 'cap.surface', 'cap.color','does.bruise.or.bleed',
                    'gill.attachment', 'gill.color','stem.color', 'has.ring',
                    'ring.type', 'habitat', 'season']

# onehot_encoder = OneHotEncoder(sparse=False)
# X_encoded = onehot_encoder.fit_transform(X[categorical_cols])

# # convert encoded data to a dataframe

# data_hot_encoded = pd.DataFrame(X_encoded, index=X.index)

# data_with_numerical_cols = X.drop(columns=categorical_cols)

# # concat numerical data and the hot encoded data 
# output = pd.concat([data_hot_encoded, data_with_numerical_cols], axis=1)

# output.head()

# try get_dummies
df_dummies = pd.get_dummies(X, columns = categorical_cols)
df_dummies.head()

# scale the data
X_scaled = StandardScaler().fit_transform(df_dummies)
X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=df_dummies.columns)

# train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.20, random_state=42)


# label encode response variable
label_encoder = LabelEncoder()
label_encoder.fit(y_train)
y_train = label_encoder.transform(y_train)
y_test = label_encoder.transform(y_test)

# check actual label mapping for Recall
print(X_train.shape)
# print(list(y_test))
# print(list(label_encoder.inverse_transform(y_test)))

X_test.shape

df.hist(bins=30, figsize=(15, 10), color = "purple", density=True )
plt.tight_layout()
plt.show()
plt.savefig('/content/drive/MyDrive/DMSL project/MushroomDataset/nominalDistribution.png', transparent=True)

df_imputted[['stem.color','class']].value_counts()

"""# Elastic Net"""

def run_elastic_net(x_train, y_train):

  # Train model and fit
  elastic_model = ElasticNetCV(cv=10, random_state=42).fit(x_train, y_train)

  # perform permutation importance
  results = permutation_importance(elastic_model, x_train, y_train, scoring='neg_mean_squared_error')

  # get importance
  importance = results.importances_mean

  # summarize feature importance
  # for feature_ind, score in enumerate(importance):
  #   print('Feature: %0d, Score: %.5f' % (feature_ind, score))

  
  # create an ordered dict
  from collections import OrderedDict
  from operator import itemgetter

  l = OrderedDict(sorted(enumerate(importance), key=itemgetter(1), reverse=True))

  print("Feature Importance using Negative MSE score\n")
  for key, val in l.items():
      print('Feature: %0d, Score: %.5f' % (key, val))
  
  # plot feature importance
  figure(figsize=(8, 6), dpi=80)
  plt.bar([x for x in range(len(importance))], importance, align='center', color='limegreen')
  plt.ylabel("Negative MSE score")
  plt.xlabel('Features')
  plt.title("Feature importance using Elastic Net")
  plt.show()
  plt.close()

# run_elastic_net(X_train, y_train)

"""# Modeling on full dataset"""

from sklearn.model_selection import RepeatedStratifiedKFold
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)

def run_random_forest(X_train, y_train):

    # import packages
    from sklearn.ensemble import RandomForestClassifier


    # Train Model
    clf = RandomForestClassifier()
    n_trees = list(np.arange(1,201,2))
    clf_gs = GridSearchCV(estimator=clf,
                        param_grid={'n_estimators':n_trees},
                        scoring='recall', cv=cv, n_jobs=-1)
    

    # Return fitted model
    return clf.fit(X_train,y_train)

def run_svm(X_train, y_train):

    from sklearn.svm import SVC
    # Create Model
    clf = SVC(probability=True)

    # Train Model
    clf_gs = GridSearchCV(estimator=clf,
                        param_grid={'C':[0.1,1,10,100],'gamma':['scale','auto'], 'kernel':['rbf','sigmoid']},
                        scoring='recall', cv=cv, n_jobs=-1)
    

    # Fit Model
    return clf.fit(X_train,y_train)

def run_xgboost(X_train, y_train):
    from xgboost import XGBClassifier

    # define the model
    clf = XGBClassifier(feature_names=X_train.columns)

    # Train Model
    params = {
            'min_child_weight': [1, 5, 10],
            'gamma': [0.5, 1, 1.5, 2, 5],
            'subsample': [0.6, 0.8, 1.0],
            'colsample_bytree': [0.6, 0.8, 1.0],
            'max_depth': [3, 4, 5]
            }

    clf_gs = GridSearchCV(estimator=clf,
                        param_grid= params,
                        scoring='recall',cv=cv, n_jobs=-1)
    

    # Fit Model
    return clf.fit(X_train,y_train)

def run_logit(X_train, y_train):

    from sklearn.linear_model import LogisticRegression

    # Train Model
    clf = LogisticRegression()
    C_params = np.linspace(0.001, 2, 100)
    clf_gs = GridSearchCV(estimator=clf,
                            param_grid={'penalty':['l1','l2'],'C':C_params},
                            scoring='recall', cv=cv, n_jobs=-1)

    # Fit Model
    return clf.fit(X_train,y_train)

def run_neural_net(X_train, y_train):
  from sklearn.neural_network import MLPClassifier
  parameters = {'alpha': 10.0 ** -np.arange(1, 7), 'hidden_layer_sizes':np.arange(10, 40), 
                'activation': ['logistic', 'relu'], 'learning_rate_init' : [0.05]}

  clf = MLPClassifier()

  # clf_gs = GridSearchCV(clf, parameters, cv=cv, n_jobs=-1)

  return clf.fit(X_train,y_train)



def run_lda(X_train, y_train):
  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

  # Train Model
  clf = LDA(n_components=None, priors=None, shrinkage=None, solver='svd', store_covariance=False, tol=0.0001) # need to do grid search 

  return clf.fit(X_train, y_train)

def get_accuracy(model_name, model, X_test, y_test):
    pred_y = model.predict(X_test)
    model_accuracy = np.round(accuracy_score(y_test, pred_y),4)
    cm = confusion_matrix(y_test, pred_y)
    precision,recall,fscore,support = score(y_true=y_test,y_pred=pred_y)

    print('Model results for {}'.format(model_name))
    print('Accuracy Score: {}'.format(model_accuracy))
    print('Confusion Matrix: \n {}'.format(cm))
    
    class_list = [x for x in range(0, 2)]
    data_dict = {'Class':class_list,'Precision':precision,'Recall':recall,'F-1 Score':fscore}
    model_dataframe = pd.DataFrame(data_dict)
    if model_name == 'Neural Network':
        print(pred_y.cv_results_)
    print(model_dataframe)
    print('-------------------------------')
    
    return model_dataframe

# Run  Model
rf = run_random_forest(X_train, np.ravel(y_train))

# Print Results
get_accuracy('Random Forest:', rf, X_test, y_test)

feat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)
feat_importances.nlargest(25).argsort()[::-1].plot(kind='barh', fontsize=16)
plt.show()

# Run  Model
lda = run_lda(X_train, np.ravel(y_train))

# Print Results
get_accuracy('lda:', lda, X_test, y_test)

# Run  Model
logit = run_logit(X_train, np.ravel(y_train))

# Print Results
get_accuracy('Logistic Regression:', logit, X_test, y_test)

# Run  Model
svm = run_svm(X_train, np.ravel(y_train))

# Print Results
get_accuracy('SVM:', svm, X_test, y_test)

# show tree from random forest
# rf_estimators = rf.estimators_[0]
# dot_data = export_graphviz(rf_estimators, out_file=None, 
#                 feature_names = X_scaled_df.columns,
#                 class_names = ['edible','poisonous'],
#                 rounded = True, proportion = False, 
#                 precision = 1, filled = True)
# graph=pydot.graph_from_dot_data(dot_data)
# Image(graph[0].create_png())

# Run  Model
xgboost = run_xgboost(X_train, np.ravel(y_train))

# Print Results
get_accuracy('XGBoost:', xgboost, X_test, y_test)

from xgboost import plot_importance
plot_importance(xgboost, max_num_features=10)

# Run  Model
nn = run_neural_net(X_train, np.ravel(y_train))

# Print Results
get_accuracy('Neural Network:', nn, X_test, y_test)

## AUC curve
def plot_roc(model, X_test, y_test, plot_title):
  y_pred_proba = model.predict_proba(X_test)[::,1]
  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
  auc = metrics.roc_auc_score(y_test, y_pred_proba)

  figure(figsize=(3,3))
  # create ROC curve
  lw = 2
  figure(figsize=(8, 6), dpi=80)
  plt.plot(fpr,tpr,lw=lw, label='ROC curve (area = %0.4f)' %auc, color='darkorange')
  plt.plot([0,1],[0,1],'g--')
  plt.xlabel("False Positive Rate")
  plt.ylabel("True Positive Rate")
  plt.title(plot_title)
  plt.legend(loc=4)
  plt.show()

plot_roc(lda, X_test, y_test, "Linear Discriminant Analysis (full dataset)")

plot_roc(logit, X_test, y_test, "Logistic Regression (full dataset)")

plot_roc(svm, X_test, y_test, "Support Vector Machines (full dataset)")

plot_roc(rf, X_test, y_test, "Random Forest (full dataset)")

plot_roc(xgboost, X_test, y_test, "Extreme Gradient Boosting (full dataset)")

plot_roc(nn, X_test, y_test, "Neural Networks (full dataset)")

"""# Feature Importance Computed with SHAP Values"""

# pip install shap

# import shap
# explainer = shap.TreeExplainer(rf)
# shap_values = explainer.shap_values(X_test)
# shap.summary_plot(shap_values, X_test, plot_type="bar")
# plt.show()
# plt.savefig('/content/drive/MyDrive/DMSL project/MushroomDataset/shapColumns.png', transparent=True)

shap_columns = ['stem.color_white','stem.width','stem.height','cap.diameter','gill.attachment_pores','gill.color_white','cap.shape_bell','gill.attachment_adnate','ring.type_zone','cap.color_red','stem.color_yellow','cap.shape_convex','has.ring_ring','cap.surface_silky',
                'does.bruise.or.bleed_yes','gill.attachment_free','gill.color_yellow','cap.surface_sticky','cap.surface_smooth']



# explainer_xgboost = shap.TreeExplainer(xgboost)
# shap_values_xgboost = explainer_xgboost.shap_values(X_test)
# shap.summary_plot(shap_values_xgboost, X_test, plot_type="bar")

shap_columns_xgboost = ['stem.color_white','stem.width','stem.height','gill.attachment_pores','ring.type_zone','gill.color_white','cap.shape_bell','cap.surface_silky','cap.surface_sticky','cap.color_brown','season_Winter','cap.color_red','does.bruise.or.bleed_no',
                        'cap.diameter','cap.surface_fibrous','cap.color_green','cap.surface_smooth','stem.color_gray','gill.color_yellow','has.ring_none']

"""# Feature Selection using PowerShap (Shapley values)"""

# from sklearn.linear_model import LogisticRegressionCV
# selector = PowerShap(model = LogisticRegressionCV(max_iter=100), automatic=True, limit_automatic=50)
# selector.fit(X_train, y_train)

# selector.transform(X_test)

# selector._processed_shaps_df

# selector2 = PowerShap(
#     model = GradientBoostingClassifier(),
#     automatic=True, limit_automatic=100)
# selector2.fit(X_train, y_train)
# selector2.transform(X_test)
# selector2._processed_shaps_df

# powerShap_df = selector._processed_shaps_df
# powerShap_df2 = selector2._processed_shaps_df

# powerShap_df = powerShap_df.sort_values('impact', ascending=False)
# powerShap_df2 = powerShap_df2.sort_values('impact', ascending=False)
# powerShap_df = powerShap_df.head(25)
# powerShap_df2 = powerShap_df2.head(10)

# powerShap_cols1 = set(powerShap_df.index.tolist())
# powerShap_cols = powerShap_df2.index.tolist()
# powerShap_cols = powerShap_cols1.intersection(powerShap_cols2)
# powerShap_cols

"""# Modeling with grid search"""

# def run_rf_gs(X_train, y_train):

#     # import packages
#     from sklearn.ensemble import RandomForestClassifier


#     # Train Model
#     clf = RandomForestClassifier()
#     n_trees = list(np.arange(1,201,2))
#     clf_gs = GridSearchCV(estimator=clf,
#                         param_grid={'n_estimators':n_trees},
#                         scoring='recall', cv=cv, n_jobs=-1)
    

#     # Return fitted model
#     return clf_gs.fit(X_train,y_train)

# def run_svm_gs(X_train, y_train):

#     from sklearn.svm import SVC
#     # Create Model
#     clf = SVC()

#     # Train Model
#     clf_gs = GridSearchCV(estimator=clf,
#                         param_grid={'C':[0.1,1,10,100],'gamma':['scale','auto'], 'kernel':['rbf','sigmoid']},
#                         scoring='recall', cv=cv, n_jobs=-1)
    

#     # Fit Model
#     return clf_gs.fit(X_train,y_train)

# def run_xgboost_gs(X_train, y_train):
#     from xgboost import XGBClassifier

#     # define the model
#     clf = XGBClassifier(feature_names=X_train.columns)

#     # Train Model
#     params = {
#             'min_child_weight': [1, 5, 10],
#             'gamma': [0.5, 1, 1.5, 2, 5],
#             'subsample': [0.6, 0.8, 1.0],
#             'colsample_bytree': [0.6, 0.8, 1.0],
#             'max_depth': [3, 4, 5]
#             }

#     clf_gs = GridSearchCV(estimator=clf,
#                         param_grid= params,
#                         scoring='recall',cv=cv, n_jobs=-1)
    

#     # Fit Model
#     return clf_gs.fit(X_train,y_train)

# def run_logit_gs(X_train, y_train):

#     from sklearn.linear_model import LogisticRegression

#     # Train Model
#     clf = LogisticRegression()
#     C_params = np.linspace(0.001, 2, 100)
#     clf_gs = GridSearchCV(estimator=clf,
#                             param_grid={'penalty':['l1','l2'],'C':C_params},
#                             scoring='recall', cv=cv, n_jobs=-1)

#     # Fit Model
#     return clf_gs.fit(X_train,y_train)

# def run_nn_gs(X_train, y_train):
#   from sklearn.neural_network import MLPClassifier
#   parameters = {'alpha': 10.0 ** -np.arange(1, 7), 'hidden_layer_sizes':np.arange(10, 40), 
#                 'activation': ['logistic', 'relu'], 'learning_rate_init' : [0.05]}

#   clf = MLPClassifier()

#   clf_gs = GridSearchCV(clf, parameters, cv=cv, n_jobs=-1)

#   return clf_gs.fit(X_train,y_train)

# # # Run  Model
# rf_gs = run_rf_gs(X_train, np.ravel(y_train))

# # # Print Results
# get_accuracy('Random Forest gs:', rf_gs, X_test, y_test)

# # Run  Model
# logit_gs = run_logit_gs(X_train, np.ravel(y_train))

# # Print Results
# get_accuracy('Logistic Regression gs:', logit_gs, X_test, y_test)

# # Run  Model
# svm_gs = run_svm_gs(X_train, np.ravel(y_train))

# # Print Results
# get_accuracy('SVM with Grid Search:', svm_gs, X_test, y_test)

# Run  Model
# xgboost_gs = run_xgboost_gs(X_train, np.ravel(y_train))

# # Print Results
# get_accuracy('XGBoost with Grid Search:', xgboost_gs, X_test, y_test)

# # Run  Model
# nn_gs = run_nn_gs(X_train, np.ravel(y_train))

# # Print Results
# get_accuracy('Neural Network:', nn_gs, X_test, y_test)

"""# Modeling with subset data"""

shap_df = df_dummies[df_dummies.columns & shap_columns]
print(shap_df.shape)
shap_df.head()

# scale the data
shap_df_subset = shap_df.copy()
X_scaled_subset = StandardScaler().fit_transform(shap_df_subset)
X_scaled_subset_df = pd.DataFrame(X_scaled_subset, index=X.index, columns=shap_df.columns)

# train/test split
X_train_subset, X_test_subset, y_train_subset, y_test_subset = train_test_split(X_scaled_subset_df, y, test_size=0.20, random_state=42)

# label encode response variable
label_encoder_subset = LabelEncoder()
label_encoder_subset.fit(y_train_subset)
y_train_subset = label_encoder_subset.transform(y_train_subset)
y_test_subset = label_encoder_subset.transform(y_test_subset)

# Run  Model
lda_subset = run_lda(X_train_subset, np.ravel(y_train_subset))

# Print Results
get_accuracy('lda:', lda_subset, X_test_subset, y_test_subset)

# Run  Model
logit_subset = run_logit(X_train_subset, np.ravel(y_train_subset))

# Print Results
get_accuracy('Logistic Regression:', logit_subset, X_test_subset, y_test_subset)

# Run  Model
svm_subset = run_svm(X_train_subset, np.ravel(y_train_subset))

# Print Results
get_accuracy('SVM:', svm_subset, X_test_subset, y_test_subset)

# Run  Model
rf_subset = run_random_forest(X_train_subset, np.ravel(y_train_subset))

# Print Results
get_accuracy('Random Forest:', rf_subset, X_test_subset, y_test_subset)

# #show tree from random forest
# rf_estimators_subset = rf.estimators_[0]
# dot_data_subset = export_graphviz(rf_estimators_subset, out_file=None, 
#                 feature_names = X_scaled_subset_df.columns,
#                 class_names = ['edible','poisonous'],
#                 rounded = True, proportion = False, 
#                 precision = 1, filled = True)
# graph_subset=pydot.graph_from_dot_data(dot_data_subset)
# Image(graph_subset[0].create_png())

# Run  Model
xgboost_subset = run_xgboost(X_train_subset, np.ravel(y_train_subset))

# Print Results
get_accuracy('XGBoost:', xgboost_subset, X_test_subset, y_test_subset)

plot_importance(xgboost_subset, max_num_features=10)

# Run  Model
nn_subset = run_neural_net(X_train_subset, np.ravel(y_train_subset))

# Print Results
get_accuracy('Neural Network:', nn_subset, X_test_subset, y_test_subset)

plot_roc(lda_subset, X_test_subset, y_test_subset, "Linear Discriminant Analayis (subset dataset)")

plot_roc(logit_subset, X_test_subset, y_test_subset, "Logistic Regression (subset dataset)")

plot_roc(svm_subset, X_test_subset, y_test_subset, "Support Vector Machines (subset dataset)")

plot_roc(rf_subset, X_test_subset, y_test_subset, "Random Forest (subset dataset)")

plot_roc(xgboost_subset, X_test_subset, y_test_subset, "XGBoost (subset dataset)")

plot_roc(nn_subset, X_test_subset, y_test_subset, "Neural Network (subset dataset)")

feat_importances_subset = pd.Series(rf_subset.feature_importances_, index=X_train_subset.columns)
feat_importances_subset.nlargest(25).argsort()[::-1].plot(kind='barh', fontsize=16)
plt.show()

"""# Modeling without outliers on full dataset"""

drop_df = df_dummies.copy(deep=True)
drop_y = y.drop(y.index[outliers]).reset_index(drop=True)
drop_df = drop_df.drop(drop_df.index[outliers]).reset_index(drop=True)
print(drop_df.shape)
drop_df.head()

# scale the data
X_scaled_drop = StandardScaler().fit_transform(drop_df)
X_scaled_drop_df = pd.DataFrame(X_scaled_drop, index=drop_df.index, columns=drop_df.columns)

# train/test split
X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(X_scaled_drop_df, drop_y, test_size=0.20, random_state=42)

# label encode response variable
label_encoder_drop = LabelEncoder()
label_encoder_drop.fit(y_train_drop)
y_train_drop = label_encoder_drop.transform(y_train_drop)
y_test_drop = label_encoder_drop.transform(y_test_drop)

# Run  Model
lda_drop = run_lda(X_train_drop, np.ravel(y_train_drop))

# Print Results
get_accuracy('lda:', lda_drop, X_test_drop, y_test_drop)

logit_drop = run_logit(X_train_drop, np.ravel(y_train_drop))

get_accuracy('Logistic Regression:', logit_drop, X_test_drop, y_test_drop)

# Run  Model
svm_drop = run_svm(X_train_drop, np.ravel(y_train_drop))

# Print Results
get_accuracy('SVM:', svm_drop, X_test_drop, y_test_drop)

# Run  Model
rf_drop = run_random_forest(X_train_drop, np.ravel(y_train_drop))

# Print Results
get_accuracy('Random Forest:', rf_drop, X_test_drop, y_test_drop)

# show tree from random forest
# rf_estimators_drop = rf_drop.estimators_[0]
# dot_data_drop = export_graphviz(rf_estimators_drop, out_file=None, 
#                 feature_names = X_scaled_drop_df.columns,
#                 class_names = ['edible','poisonous'],
#                 rounded = True, proportion = False, 
#                 precision = 1, filled = True)
# graph_drop=pydot.graph_from_dot_data(dot_data_drop)
# Image(graph_drop[0].create_png())

# Run  Model
xgboost_drop = run_xgboost(X_train_drop, np.ravel(y_train_drop))

# Print Results
get_accuracy('XGBoost:', xgboost_drop, X_test_drop, y_test_drop)

plot_importance(xgboost_drop, max_num_features=10)

# Run  Model
nn_drop = run_neural_net(X_train_drop, np.ravel(y_train_drop))

# Print Results
get_accuracy('Neural Network:', nn_drop, X_test_drop, y_test_drop)

plot_roc(lda_drop, X_test_drop, y_test_drop, "Linear Discriminant Analayis (full dataset without outliers)")

plot_roc(logit_drop, X_test_drop, y_test_drop, "Logistic Regression (full dataset without outliers)")

plot_roc(svm_drop, X_test_drop, y_test_drop, "Support Vector Machines (full dataset without outliers)")

plot_roc(rf_drop, X_test_drop, y_test_drop, "Random Forest (full dataset without outliers)")

plot_roc(xgboost_drop, X_test_drop, y_test_drop, "XGBoost (full dataset without outliers)")

plot_roc(nn_drop, X_test_drop, y_test_drop, "Neural Network (full dataset without outliers)")

feat_importances_drop = pd.Series(rf_drop.feature_importances_, index=X_train_drop.columns)
feat_importances_drop.nlargest(25).argsort()[::-1].plot(kind='barh', fontsize=16)
plt.show()

"""# Modeling on subset data without outliers"""

clean_df = df_dummies[df_dummies.columns & shap_columns]
clean_df = clean_df.drop(clean_df.index[outliers]).reset_index(drop=True)
print(clean_df.shape)
clean_df.head()

# scale the data
X_scaled_clean = StandardScaler().fit_transform(clean_df)
X_scaled_clean_df = pd.DataFrame(X_scaled_clean, index=clean_df.index, columns=clean_df.columns)

# train/test split
X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_scaled_clean_df, drop_y, test_size=0.20, random_state=42)

# label encode response variable
label_encoder_clean = LabelEncoder()
label_encoder_clean.fit(y_train_clean)
y_train_clean = label_encoder_clean.transform(y_train_clean)
y_test_clean = label_encoder_clean.transform(y_test_clean)



# Run  Model
lda_clean = run_lda(X_train_clean, np.ravel(y_train_clean))

# Print Results
get_accuracy('lda:', lda_clean, X_test_clean, y_test_clean)

# Run  Model
logit_clean = run_logit(X_train_clean, np.ravel(y_train_clean))

# Print Results
get_accuracy('Logistic Regression:', logit_clean, X_test_clean, y_test_clean)

# Run  Model
svm_clean = run_svm(X_train_clean, np.ravel(y_train_clean))

# Print Results
get_accuracy('SVM:', svm_clean, X_test_clean, y_test_clean)

# Run  Model
rf_clean = run_random_forest(X_train_clean, np.ravel(y_train_clean))

# Print Results
get_accuracy('Random Forest:', rf_clean, X_test_clean, y_test_clean)

# show tree from random forest
# rf_estimators_clean = rf_clean.estimators_[0]
# dot_data_clean = export_graphviz(rf_estimators_clean, out_file=None, 
#                 feature_names = X_scaled_clean_df.columns,
#                 class_names = ['edible','poisonous'],
#                 rounded = True, proportion = False, 
#                 precision = 1, filled = True)
# graph_clean=pydot.graph_from_dot_data(dot_data_clean)
# Image(graph_clean[0].create_png())

# Run  Model
xgboost_clean = run_xgboost(X_train_clean, np.ravel(y_train_clean))

# Print Results
get_accuracy('XGBoost:', xgboost_clean, X_test_clean, y_test_clean)

plot_importance(xgboost_clean, max_num_features=10)
plt.show()

# Run  Model
nn_clean = run_neural_net(X_train_clean, np.ravel(y_train_clean))

# Print Results
get_accuracy('Neural Network:', nn_clean, X_test_clean, y_test_clean)

plot_roc(lda_clean, X_test_clean, y_test_clean, "Linear Discriminant Analayis (subset dataset without outliers)")

plot_roc(logit_clean, X_test_clean, y_test_clean, "Logistic Regression (subset dataset without outliers)")

plot_roc(svm_clean, X_test_clean, y_test_clean, "Support Vector Machines (subset dataset without outliers)")

plot_roc(rf_clean, X_test_clean, y_test_clean, "Random Forest (subset dataset without outliers)")

plot_roc(xgboost_clean, X_test_clean, y_test_clean, "XGBoost (subset dataset without outliers)")

plot_roc(nn_clean, X_test_clean, y_test_clean, "Neural Network (subset dataset without outliers)")

feat_importances_clean = pd.Series(rf_clean.feature_importances_, index=X_train_clean.columns)
feat_importances_clean.nlargest(25).argsort()[::-1].plot(kind='barh', fontsize=16)
plt.show()

"""# Feature Selection Using PowerSHAP Without Outliers"""

# clean_df2 = df_dummies[df_dummies.columns & powerShap_cols]
# clean_df2 = clean_df2.drop(clean_df2.index[outliers]).reset_index(drop=True)
# print(clean_df2.shape)
# clean_df2.head()

# # scale the data
# X_scaled_clean2 = StandardScaler().fit_transform(clean_df2)
# X_scaled_clean_df2 = pd.DataFrame(X_scaled_clean2, index=clean_df2.index, columns=clean_df2.columns)

# # train/test split
# X_train_clean2, X_test_clean2, y_train_clean2, y_test_clean2 = train_test_split(X_scaled_clean_df2, drop_y, test_size=0.20, random_state=42)

# # label encode response variable
# label_encoder_clean2 = LabelEncoder()
# label_encoder_clean2.fit(y_train_clean2)
# y_train_clean2 = label_encoder_clean2.transform(y_train_clean2)
# y_test_clean2 = label_encoder_clean2.transform(y_test_clean2)

# # Run  Model
# lda_clean2 = run_lda(X_train_clean2, np.ravel(y_train_clean2))

# # Print Results
# get_accuracy('lda:', lda_clean2, X_test_clean2, y_test_clean2)

# # Run  Model
# logit_clean2 = run_logit(X_train_clean2, np.ravel(y_train_clean2))

# # Print Results
# get_accuracy('Logistic Regression:', logit_clean2, X_test_clean2, y_test_clean2)

# # Run  Model
# svm_clean2 = run_svm(X_train_clean2, np.ravel(y_train_clean2))

# # Print Results
# get_accuracy('SVM:', svm_clean2, X_test_clean2, y_test_clean2)

# # Run  Model
# rf_clean2 = run_random_forest(X_train_clean2, np.ravel(y_train_clean2))

# # Print Results
# get_accuracy('Random Forest:', rf_clean2, X_test_clean2, y_test_clean2)

# show tree from random forest
# rf_estimators_clean2 = rf_clean2.estimators_[0]
# dot_data_clean2 = export_graphviz(rf_estimators_clean2, out_file=None, 
#                 feature_names = X_scaled_clean_df2.columns,
#                 class_names = ['edible','poisonous'],
#                 rounded = True, proportion = False, 
#                 precision = 1, filled = True)
# graph_clean2=pydot.graph_from_dot_data(dot_data_clean2)
# Image(graph_clean2[0].create_png())

# # Run  Model
# xgboost_clean2 = run_xgboost(X_train_clean2, np.ravel(y_train_clean2))

# # Print Results
# get_accuracy('XGBoost:', xgboost_clean2, X_test_clean2, y_test_clean2)

# plot_importance(xgboost_clean2, max_num_features=5)

# # Run  Model
# nn_clean2 = run_neural_net(X_train_clean2, np.ravel(y_train_clean2))

# # Print Results
# get_accuracy('Neural Network:', nn_clean2, X_test_clean2, y_test_clean2)

# plot_roc(lda_clean2, X_test_clean2, y_test_clean2, "Linear discriminat Analysis (PowerShap featues without outliers)")

# plot_roc(logit_clean2, X_test_clean2, y_test_clean2, "Logistic Regression (PowerShap featues without outliers)")

# plot_roc(svm_clean2, X_test_clean2, y_test_clean2, "Support Vector Machines (PowerShap featues without outliers)")

# plot_roc(rf_clean2, X_test_clean2, y_test_clean2, "Random Forest (PowerShap featues without outliers)")

# plot_roc(xgboost_clean2, X_test_clean2, y_test_clean2, "XGBoost (PowerShap featues without outliers)")

# plot_roc(nn_clean2, X_test_clean2, y_test_clean2, "Neural Network (PowerShap featues without outliers)")

# feat_importances_clean2 = pd.Series(rf_clean2.feature_importances_, index=X_train_clean2.columns)
# feat_importances_clean2.nlargest(15).argsort()[::-1].plot(kind='barh')

list1 = feat_importances[0:24]
list2 = feat_importances_subset[0:24]
list3 = feat_importances_drop[0:24]
list4 = feat_importances_clean[0:24]
list5 = list1+list2+list3+list4

list5.nlargest(20).argsort()[::-1].plot(kind='barh')

